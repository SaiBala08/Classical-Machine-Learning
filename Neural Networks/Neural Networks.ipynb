{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Udemy single layer NN XOR Problem using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyabalasankula/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.2873 - acc: 0.5000\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.2871 - acc: 0.5000\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.2868 - acc: 0.5000\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2865 - acc: 0.5000\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.2862 - acc: 0.5000\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.2859 - acc: 0.5000\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.2856 - acc: 0.5000\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.2854 - acc: 0.5000\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.2851 - acc: 0.5000\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.2848 - acc: 0.5000\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.2845 - acc: 0.5000\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.2842 - acc: 0.5000\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.2840 - acc: 0.5000\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.2837 - acc: 0.5000\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.2834 - acc: 0.5000\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.2831 - acc: 0.5000\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2829 - acc: 0.5000\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2826 - acc: 0.5000\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.2823 - acc: 0.5000\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.2820 - acc: 0.5000\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.2818 - acc: 0.5000\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.2815 - acc: 0.5000\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.2812 - acc: 0.5000\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.2810 - acc: 0.5000\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.2807 - acc: 0.5000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.2805 - acc: 0.5000\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.2802 - acc: 0.5000\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.2799 - acc: 0.5000\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.2797 - acc: 0.5000\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.2794 - acc: 0.5000\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.2792 - acc: 0.5000\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.2789 - acc: 0.5000\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.2787 - acc: 0.5000\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.2784 - acc: 0.5000\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.2782 - acc: 0.5000\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.2779 - acc: 0.5000\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.2777 - acc: 0.5000\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.2775 - acc: 0.5000\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.2772 - acc: 0.5000\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.2770 - acc: 0.5000\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.2767 - acc: 0.5000\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.2765 - acc: 0.5000\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.2763 - acc: 0.5000\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.2760 - acc: 0.5000\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.2758 - acc: 0.5000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.2756 - acc: 0.5000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.2753 - acc: 0.5000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.2751 - acc: 0.5000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.2749 - acc: 0.5000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.2747 - acc: 0.5000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.2744 - acc: 0.5000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.2742 - acc: 0.5000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.2740 - acc: 0.5000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.2738 - acc: 0.5000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2736 - acc: 0.5000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.2733 - acc: 0.5000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2731 - acc: 0.5000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2729 - acc: 0.5000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2727 - acc: 0.5000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2725 - acc: 0.5000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2723 - acc: 0.5000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2721 - acc: 0.5000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2719 - acc: 0.5000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2717 - acc: 0.5000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2715 - acc: 0.5000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2713 - acc: 0.5000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2711 - acc: 0.5000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2709 - acc: 0.5000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2707 - acc: 0.5000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2705 - acc: 0.5000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.2703 - acc: 0.5000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.2701 - acc: 0.5000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.2699 - acc: 0.5000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.2697 - acc: 0.5000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2695 - acc: 0.5000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2693 - acc: 0.5000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2691 - acc: 0.5000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2689 - acc: 0.5000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2688 - acc: 0.5000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2686 - acc: 0.5000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2684 - acc: 0.5000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2682 - acc: 0.5000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2680 - acc: 0.5000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2679 - acc: 0.5000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2677 - acc: 0.5000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2675 - acc: 0.5000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2673 - acc: 0.5000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2672 - acc: 0.5000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2670 - acc: 0.5000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2668 - acc: 0.5000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2667 - acc: 0.5000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2665 - acc: 0.5000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2663 - acc: 0.5000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2662 - acc: 0.5000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2660 - acc: 0.5000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2658 - acc: 0.5000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2657 - acc: 0.5000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2655 - acc: 0.5000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2654 - acc: 0.5000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2652 - acc: 0.5000\n",
      "[[0.6240114 ]\n",
      " [0.6281252 ]\n",
      " [0.61167747]\n",
      " [0.61787367]]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(4,input_dim = 2, activation = 'sigmoid'))\n",
    "model.add(Dense(1,input_dim = 4, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X,Y, epochs = 100, verbose = 2)\n",
    "\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Udemy single layer NN Iris data set Problem using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "features = iris_data.data\n",
    "labels = iris_data.target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "target = encoder.fit_transform(labels).toarray()\n",
    "\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size = 0.25)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = 4, activation = 'sigmoid'))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "optimizer  = adam(lr = 0.001)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "model.fit(features,target, epochs = 1000, batch_size = 20, verbose = 2)\n",
    "\n",
    "model.evaluate(feature_test,target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Andrew NG Neural Network weekly Assisgnment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " ...\n",
      " [8]\n",
      " [9]\n",
      " [8]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "\n",
    "input_layer_size  = 64; \n",
    "hidden_layer_size = 25;   \n",
    "num_labels = 10;\n",
    "\n",
    "print(X[0])\n",
    "print(digits.target.reshape(-1,1))\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randinWeights(r,c):\n",
    "    \n",
    "    e = 0.12\n",
    "    \n",
    "    return np.random.rand(r, 1+c)*(2*e) - e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randinWeights(hidden_layer_size, input_layer_size);\n",
    "initial_Theta2 = randinWeights(num_labels, hidden_layer_size);\n",
    "\n",
    "initial_nn_params = [initial_Theta1, initial_Theta2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNCostGradFunc(nn_par, i_layer_size, h_layer_size, num_labels, X,Y,lamda):\n",
    "    \n",
    "    Theta1 = nn_par[0]\n",
    "    Theta2 = nn_par[1]\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    X = np.insert(X,0, np.ones((m)), axis = 1)\n",
    "    \n",
    "    a1 = X\n",
    "    z2 = a1@Theta1.T\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.insert(a2,0, np.ones((m)), axis = 1)\n",
    "    a3 = sigmoid(a2@Theta2.T)\n",
    "    \n",
    "    ry = np.identity(num_labels)[Y,:]\n",
    "\n",
    "    cost = ry.T@np.log(a3) + (1 - ry).T@np.log(1 - a3)\n",
    "    J = -sum(sum(cost)) / m\n",
    "\n",
    "    reg = sum(sum(Theta1[:,2:]**2)) + sum(sum(Theta2[: , 2:]**2))\n",
    "\n",
    "    J = J + (lamda/(2*m))*reg\n",
    "    \n",
    "    \n",
    "    ## Back Propagation\n",
    "\n",
    "    delta3 = a3 - ry\n",
    "    delta2 = np.multiply((delta3@Theta2)[:,1:],d_sigmoid(z2))\n",
    "    \n",
    "\n",
    "    Delta1 = delta2.T@a1\n",
    "    Delta2 = delta3.T@a2\n",
    "\n",
    "    Theta1_grad = Delta1/m + lamda*np.insert(Theta1[:,1:],0, np.array((h_layer_size)), axis = 1)/m\n",
    "    Theta2_grad = Delta2/m + lamda*np.insert(Theta2[:,1:],0, np.array((num_labels)), axis = 1)/m\n",
    "    \n",
    "    Theta_grad = [Theta1_grad, Theta2_grad]\n",
    "\n",
    "    return J, Theta_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.46225786296884"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J,Theta_grad = NNCostGradFunc(initial_nn_params, input_layer_size, hidden_layer_size, num_labels, X,Y,1)\n",
    "\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.9910962715637173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "lamda = 1\n",
    "\n",
    "def Train_NN(initial_nn_params, input_layer_size, hidden_layer_size, num_labels, X,Y,lamda,alpha):\n",
    "    \n",
    "    iterations = 1000\n",
    "    Theta1,Theta2 = initial_nn_params\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        J,Theta_grad = NNCostGradFunc(initial_nn_params, input_layer_size, hidden_layer_size, num_labels, X,Y,lamda)\n",
    "\n",
    "        Theta1 = Theta1 - alpha*Theta_grad[0]\n",
    "        Theta2 = Theta2 - alpha*Theta_grad[1]\n",
    "\n",
    "        initial_nn_params = [Theta1, Theta2]\n",
    "\n",
    "        #print(\"Iteration : \",i,\", Cost : \",J)\n",
    "    \n",
    "    return initial_nn_params\n",
    "\n",
    "\n",
    "def Predict(Theta1,Theta2,X,Y):\n",
    "    \n",
    "    m = X.shape[0];\n",
    "\n",
    "    h1 = sigmoid(np.insert(X,0, np.ones((m)), axis = 1) @ Theta1.T);\n",
    "    h2 = sigmoid(np.insert(h1,0, np.ones((m)), axis = 1) @ Theta2.T);\n",
    "    \n",
    "    p = np.argmax(h2, axis=1)\n",
    "    \n",
    "    print(\"Accuracy \",np.count_nonzero((p - Y) == 0)/len(Y))\n",
    "    \n",
    "    return p\n",
    "   \n",
    "        \n",
    "        \n",
    "Theta1, Theta2 = Train_NN(initial_nn_params, input_layer_size, hidden_layer_size, num_labels, X,Y,lamda,alpha)\n",
    "Predict(Theta1,Theta2,X,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "Epoch 1/100\n",
      " - 0s - loss: 2.3203 - acc: 0.1982\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.8088 - acc: 0.4826\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.4907 - acc: 0.6852\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.2619 - acc: 0.7780\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.0768 - acc: 0.8396\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.9237 - acc: 0.8790\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.8046 - acc: 0.8938\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.7056 - acc: 0.9206\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6225 - acc: 0.9265\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5538 - acc: 0.9339\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.4973 - acc: 0.9347\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.4457 - acc: 0.9495\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.4078 - acc: 0.9503\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.3705 - acc: 0.9555\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.3404 - acc: 0.9636\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.3131 - acc: 0.9659\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2883 - acc: 0.9688\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2676 - acc: 0.9762\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.2482 - acc: 0.9725\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.2310 - acc: 0.9762\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.2149 - acc: 0.9785\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.2015 - acc: 0.9785\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1863 - acc: 0.9792\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.1723 - acc: 0.9837\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.1600 - acc: 0.9814\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1501 - acc: 0.9844\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1408 - acc: 0.9874\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1324 - acc: 0.9881\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1247 - acc: 0.9881\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1181 - acc: 0.9911\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1113 - acc: 0.9911\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1050 - acc: 0.9933\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0998 - acc: 0.9926\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0946 - acc: 0.9933\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0898 - acc: 0.9933\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0848 - acc: 0.9933\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0805 - acc: 0.9933\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0766 - acc: 0.9955\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0733 - acc: 0.9948\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0703 - acc: 0.9963\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0664 - acc: 0.9970\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0636 - acc: 0.9963\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0604 - acc: 0.9963\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0583 - acc: 0.9985\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0553 - acc: 0.9978\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0524 - acc: 0.9978\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0502 - acc: 0.9985\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0489 - acc: 0.9985\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0464 - acc: 0.9993\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0447 - acc: 0.9985\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0429 - acc: 0.9993\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0411 - acc: 0.9993\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0394 - acc: 0.9993\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0376 - acc: 0.9993\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0363 - acc: 0.9993\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0355 - acc: 0.9993\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0343 - acc: 0.9993\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0323 - acc: 0.9993\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0311 - acc: 0.9993\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0300 - acc: 0.9993\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0292 - acc: 0.9993\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0278 - acc: 0.9993\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0269 - acc: 0.9993\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0261 - acc: 0.9993\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0254 - acc: 0.9993\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0245 - acc: 0.9993\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0238 - acc: 0.9993\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0227 - acc: 0.9993\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0220 - acc: 0.9993\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0212 - acc: 0.9993\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0204 - acc: 0.9993\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0198 - acc: 0.9993\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0187 - acc: 0.9993\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0180 - acc: 0.9993\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0175 - acc: 0.9993\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0169 - acc: 0.9993\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0164 - acc: 0.9993\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0159 - acc: 0.9993\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0151 - acc: 0.9993\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0147 - acc: 0.9993\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0142 - acc: 0.9993\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0135 - acc: 0.9993\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0130 - acc: 0.9993\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0126 - acc: 0.9993\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0119 - acc: 0.9993\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0114 - acc: 0.9993\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0111 - acc: 0.9993\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0107 - acc: 0.9993\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0068 - acc: 1.0000\n",
      "450/450 [==============================] - 0s 190us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_11_input to have shape (64,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ae5139252c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_11_input to have shape (64,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "target = encoder.fit_transform(Y.reshape(-1,1)).toarray()\n",
    "print(target)\n",
    "\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(X, target, test_size = 0.25)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25,input_dim = 64, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "optimizer  = adam(lr = 0.001)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "model.fit(feature_train,target_train, epochs = 100, batch_size = 20, verbose = 2)\n",
    "\n",
    "model.evaluate(feature_test,target_test)\n",
    "model.predict(feature_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
